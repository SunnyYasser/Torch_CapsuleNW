{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(s, dim=-1):\n",
    "    squared_norm = torch.sum(s**2, dim=dim, keepdim=True)\n",
    "    return squared_norm / (1 + squared_norm) * s / (torch.sqrt(squared_norm) + 1e-8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2494, 0.4989, 0.7483], dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3],dtype = float)\n",
    "squash(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCapsules(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dim_caps,kernel_size=9, stride=2, padding=0):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the layer.\n",
    "        Args:\n",
    "            in_channels: \tNumber of input channels.\n",
    "            out_channels: \tNumber of output channels.\n",
    "            dim_caps:\t\tDimensionality, i.e. length, of the output capsule vector.\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        super(PrimaryCapsules, self).__init__()\n",
    "        self.dim_caps = dim_caps\n",
    "        self._caps_channel = int(out_channels / dim_caps)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = out.view(out.size(0), self._caps_channel, out.size(2), out.size(3), self.dim_caps)\n",
    "        out = out.view(out.size(0), -1, self.dim_caps)\n",
    "        return squash(out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCapsules(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dim_caps,kernel_size=9, stride=2, padding=0):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the layer.\n",
    "        Args:\n",
    "            in_channels: \tNumber of input channels.\n",
    "            out_channels: \tNumber of output channels.\n",
    "            dim_caps:\t\tDimensionality, i.e. length, of the output capsule vector.\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        super(PrimaryCapsules, self).__init__()\n",
    "        self.dim_caps = dim_caps\n",
    "        self._caps_channel = int(out_channels / dim_caps)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = out.view(out.size(0), self._caps_channel, out.size(2), out.size(3), self.dim_caps)\n",
    "        out = out.view(out.size(0), -1, self.dim_caps)\n",
    "        return squash(out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoutingCapsules(nn.Module):\n",
    "    def __init__(self, in_dim, in_caps, num_caps, dim_caps, num_routing, device: torch.device):\n",
    "       \n",
    "        \"\"\"\n",
    "        Initialize the layer.\n",
    "        Args:\n",
    "            in_dim: \t\tDimensionality (i.e. length) of each capsule vector.\n",
    "            in_caps: \t\tNumber of input capsules if digits layer.\n",
    "            num_caps: \t\tNumber of capsules in the capsule layer\n",
    "            dim_caps: \t\tDimensionality, i.e. length, of the output capsule vector.\n",
    "            num_routing:\tNumber of iterations during routing algorithm\t\t\n",
    "    \n",
    "        \"\"\"\n",
    "    \n",
    "        super(RoutingCapsules, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.in_caps = in_caps\n",
    "        self.num_caps = num_caps\n",
    "        self.dim_caps = dim_caps\n",
    "        self.num_routing = num_routing\n",
    "        self.device = device\n",
    "\n",
    "        self.W = nn.Parameter( 0.01 * torch.randn(1, num_caps, in_caps, dim_caps, in_dim ) )\n",
    "\n",
    "    def __repr__(self):\n",
    "        tab = '  '\n",
    "        line = '\\n'\n",
    "        next = ' -> '\n",
    "        res = self.__class__.__name__ + '('\n",
    "        res = res + line + tab + '(' + str(0) + '): ' + 'CapsuleLinear('\n",
    "        res = res + str(self.in_dim) + ', ' + str(self.dim_caps) + ')'\n",
    "        res = res + line + tab + '(' + str(1) + '): ' + 'Routing('\n",
    "        res = res + 'num_routing=' + str(self.num_routing) + ')'\n",
    "        res = res + line + ')'\n",
    "        return res\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # (batch_size, in_caps, in_dim) -> (batch_size, 1, in_caps, in_dim, 1)\n",
    "        x = x.unsqueeze(1).unsqueeze(4)\n",
    "        #\n",
    "        # W @ x =\n",
    "        # (1, num_caps, in_caps, dim_caps, in_dim) @ (batch_size, 1, in_caps, in_dim, 1) =\n",
    "        # (batch_size, num_caps, in_caps, dim_caps, 1)\n",
    "        u_hat = torch.matmul(self.W, x)\n",
    "        # (batch_size, num_caps, in_caps, dim_caps)\n",
    "        u_hat = u_hat.squeeze(-1)\n",
    "        # detach u_hat during routing iterations to prevent gradients from flowing\n",
    "        temp_u_hat = u_hat.detach()\n",
    "\n",
    "        '''\n",
    "        Procedure 1: Routing algorithm\n",
    "        '''\n",
    "        b = torch.zeros(batch_size, self.num_caps, self.in_caps, 1).to(self.device)\n",
    "\n",
    "        for route_iter in range(self.num_routing-1):\n",
    "            # (batch_size, num_caps, in_caps, 1) -> Softmax along num_caps\n",
    "            c = F.softmax(b, dim=1)\n",
    "\n",
    "            # element-wise multiplication\n",
    "            # (batch_size, num_caps, in_caps, 1) * (batch_size, in_caps, num_caps, dim_caps) ->\n",
    "            # (batch_size, num_caps, in_caps, dim_caps) sum across in_caps ->\n",
    "            # (batch_size, num_caps, dim_caps)\n",
    "            s = (c * temp_u_hat).sum(dim=2)\n",
    "            # apply \"squashing\" non-linearity along dim_caps\n",
    "            v = squash(s)\n",
    "            # dot product agreement between the current output vj and the prediction uj|i\n",
    "            # (batch_size, num_caps, in_caps, dim_caps) @ (batch_size, num_caps, dim_caps, 1)\n",
    "            # -> (batch_size, num_caps, in_caps, 1)\n",
    "            uv = torch.matmul(temp_u_hat, v.unsqueeze(-1))\n",
    "            b += uv\n",
    "\n",
    "        # last iteration is done on the original u_hat, without the routing weights update\n",
    "        c = F.softmax(b, dim=1)\n",
    "        s = (c * u_hat).sum(dim=2)\n",
    "        # apply \"squashing\" non-linearity along dim_caps\n",
    "        v = squash(s)\n",
    "\n",
    "        return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
