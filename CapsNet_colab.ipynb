{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yIkl2-WDdxYL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from numpy import prod\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWqr7chqfart"
   },
   "outputs": [],
   "source": [
    "def squash(s, dim=-1):\n",
    "  \n",
    "\tsquared_norm = torch.sum(s**2, dim=dim, keepdim=True)\n",
    "\treturn squared_norm / (1 + squared_norm) * s / (torch.sqrt(squared_norm) + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aQB6-oUtfnpl"
   },
   "outputs": [],
   "source": [
    "class PrimaryCapsules(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels, dim_caps,\n",
    "\tkernel_size=9, stride=2, padding=0):\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize the layer.\n",
    "\t\tArgs:\n",
    "\t\t\tin_channels: \tNumber of input channels.\n",
    "\t\t\tout_channels: \tNumber of output channels.\n",
    "\t\t\tdim_caps:\t\tDimensionality, i.e. length, of the output capsule vector.\n",
    "\t\t\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(PrimaryCapsules, self).__init__()\n",
    "\t\tself.dim_caps = dim_caps\n",
    "\t\tself._caps_channel = int(out_channels / dim_caps)\n",
    "\t\tself.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = self.conv(x)\n",
    "\t\tout = out.view(out.size(0), self._caps_channel, out.size(2), out.size(3), self.dim_caps)\n",
    "\t\tout = out.view(out.size(0), -1, self.dim_caps)\n",
    "\t\treturn squash(out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iw3BA5OZfzxD"
   },
   "outputs": [],
   "source": [
    "class RoutingCapsules(nn.Module):\n",
    "\tdef __init__(self, in_dim, in_caps, num_caps, dim_caps, num_routing, device: torch.device):\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize the layer.\n",
    "\t\tArgs:\n",
    "\t\t\tin_dim: \t\tDimensionality (i.e. length) of each capsule vector.\n",
    "\t\t\tin_caps: \t\tNumber of input capsules if digits layer.\n",
    "\t\t\tnum_caps: \t\tNumber of capsules in the capsule layer\n",
    "\t\t\tdim_caps: \t\tDimensionality, i.e. length, of the output capsule vector.\n",
    "\t\t\tnum_routing:\tNumber of iterations during routing algorithm\t\t\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(RoutingCapsules, self).__init__()\n",
    "\t\tself.in_dim = in_dim\n",
    "\t\tself.in_caps = in_caps\n",
    "\t\tself.num_caps = num_caps\n",
    "\t\tself.dim_caps = dim_caps\n",
    "\t\tself.num_routing = num_routing\n",
    "\t\tself.device = device\n",
    "\n",
    "\t\tself.W = nn.Parameter( 0.01 * torch.randn(1, num_caps, in_caps, dim_caps, in_dim ) )\n",
    "\t\n",
    "\tdef __repr__(self):\n",
    "\t\ttab = '  '\n",
    "\t\tline = '\\n'\n",
    "\t\tnext = ' -> '\n",
    "\t\tres = self.__class__.__name__ + '('\n",
    "\t\tres = res + line + tab + '(' + str(0) + '): ' + 'CapsuleLinear('\n",
    "\t\tres = res + str(self.in_dim) + ', ' + str(self.dim_caps) + ')'\n",
    "\t\tres = res + line + tab + '(' + str(1) + '): ' + 'Routing('\n",
    "\t\tres = res + 'num_routing=' + str(self.num_routing) + ')'\n",
    "\t\tres = res + line + ')'\n",
    "\t\treturn res\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tbatch_size = x.size(0)\n",
    "\t\t# (batch_size, in_caps, in_dim) -> (batch_size, 1, in_caps, in_dim, 1)\n",
    "\t\tx = x.unsqueeze(1).unsqueeze(4)\n",
    "\t\t#\n",
    "\t\t# W @ x =\n",
    "\t\t# (1, num_caps, in_caps, dim_caps, in_dim) @ (batch_size, 1, in_caps, in_dim, 1) =\n",
    "\t\t# (batch_size, num_caps, in_caps, dim_caps, 1)\n",
    "\t\tu_hat = torch.matmul(self.W, x)\n",
    "\t\t# (batch_size, num_caps, in_caps, dim_caps)\n",
    "\t\tu_hat = u_hat.squeeze(-1)\n",
    "\t\t# detach u_hat during routing iterations to prevent gradients from flowing\n",
    "\t\ttemp_u_hat = u_hat.detach()\n",
    "\n",
    "\t\t'''\n",
    "\t\tProcedure 1: Routing algorithm\n",
    "\t\t'''\n",
    "\t\tb = torch.zeros(batch_size, self.num_caps, self.in_caps, 1).to(self.device)\n",
    "\n",
    "\t\tfor route_iter in range(self.num_routing-1):\n",
    "\t\t\t# (batch_size, num_caps, in_caps, 1) -> Softmax along num_caps\n",
    "\t\t\tc = F.softmax(b, dim=1)\n",
    "\n",
    "\t\t\t# element-wise multiplication\n",
    "\t\t\t# (batch_size, num_caps, in_caps, 1) * (batch_size, in_caps, num_caps, dim_caps) ->\n",
    "\t\t\t# (batch_size, num_caps, in_caps, dim_caps) sum across in_caps ->\n",
    "\t\t\t# (batch_size, num_caps, dim_caps)\n",
    "\t\t\ts = (c * temp_u_hat).sum(dim=2)\n",
    "\t\t\t# apply \"squashing\" non-linearity along dim_caps\n",
    "\t\t\tv = squash(s)\n",
    "\t\t\t# dot product agreement between the current output vj and the prediction uj|i\n",
    "\t\t\t# (batch_size, num_caps, in_caps, dim_caps) @ (batch_size, num_caps, dim_caps, 1)\n",
    "\t\t\t# -> (batch_size, num_caps, in_caps, 1)\n",
    "\t\t\tuv = torch.matmul(temp_u_hat, v.unsqueeze(-1))\n",
    "\t\t\tb += uv\n",
    "\t\t\n",
    "\t\t# last iteration is done on the original u_hat, without the routing weights update\n",
    "\t\tc = F.softmax(b, dim=1)\n",
    "\t\ts = (c * u_hat).sum(dim=2)\n",
    "\t\t# apply \"squashing\" non-linearity along dim_caps\n",
    "\t\tv = squash(s)\n",
    "\n",
    "\t\treturn v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3WmF8gXgRmC"
   },
   "outputs": [],
   "source": [
    "class CapsuleNetwork(nn.Module):\n",
    "\tdef __init__(self, img_shape, channels, primary_dim, num_classes, out_dim, num_routing, device: torch.device, kernel_size=9):\n",
    "\t\tsuper(CapsuleNetwork, self).__init__()\n",
    "\t\tself.img_shape = img_shape\n",
    "\t\tself.num_classes = num_classes\n",
    "\t\tself.device = device\n",
    "\n",
    "\t\tself.conv1 = nn.Conv2d(img_shape[0], channels, kernel_size, stride=1, bias=True)\n",
    "\t\tself.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\t\tself.primary = PrimaryCapsules(channels, channels, primary_dim, kernel_size)\n",
    "\t\t\n",
    "\t\tprimary_caps = int(channels / primary_dim * ( img_shape[1] - 2*(kernel_size-1) ) * ( img_shape[2] - 2*(kernel_size-1) ) / 4)\n",
    "\t\tself.digits = RoutingCapsules(primary_dim, primary_caps, num_classes, out_dim, num_routing, device=self.device)\n",
    "\n",
    "\t\tself.decoder = nn.Sequential(\n",
    "\t\t\tnn.Linear(out_dim * num_classes, 512),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\tnn.Linear(512, 1024),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\tnn.Linear(1024, int(prod(img_shape)) ),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = self.conv1(x)\n",
    "\t\tout = self.relu(out)\n",
    "\t\tout = self.primary(out)\n",
    "\t\tout = self.digits(out)\n",
    "\t\tpreds = torch.norm(out, dim=-1)\n",
    "\n",
    "\t\t# Reconstruct the *predicted* image\n",
    "\t\t_, max_length_idx = preds.max(dim=1)\t\n",
    "\t\ty = torch.eye(self.num_classes).to(self.device)\n",
    "\t\ty = y.index_select(dim=0, index=max_length_idx).unsqueeze(2)\n",
    "\n",
    "\t\treconstructions = self.decoder( (out*y).view(out.size(0), -1) )\n",
    "\t\treconstructions = reconstructions.view(-1, *self.img_shape)\n",
    "\n",
    "\t\treturn preds, reconstructions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNM_qsBYgXL1"
   },
   "outputs": [],
   "source": [
    "class MarginLoss(nn.Module):\n",
    "\tdef __init__(self, size_average=False, loss_lambda=0.5):\n",
    "\t\t'''\n",
    "\t\tMargin loss for digit existence\n",
    "\t\tEq. (4): L_k = T_k * max(0, m+ - ||v_k||)^2 + lambda * (1 - T_k) * max(0, ||v_k|| - m-)^2\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tsize_average: should the losses be averaged (True) or summed (False) over observations for each minibatch.\n",
    "\t\t\tloss_lambda: parameter for down-weighting the loss for missing digits\n",
    "\t\t'''\n",
    "\t\tsuper(MarginLoss, self).__init__()\n",
    "\t\tself.size_average = size_average\n",
    "\t\tself.m_plus = 0.9\n",
    "\t\tself.m_minus = 0.1\n",
    "\t\tself.loss_lambda = loss_lambda\n",
    "\n",
    "\tdef forward(self, inputs, labels):\n",
    "\t\tL_k = labels * F.relu(self.m_plus - inputs)**2 + self.loss_lambda * (1 - labels) * F.relu(inputs - self.m_minus)**2\n",
    "\t\tL_k = L_k.sum(dim=1)\n",
    "\n",
    "\t\tif self.size_average:\n",
    "\t\t\treturn L_k.mean()\n",
    "\t\telse:\n",
    "\t\t\treturn L_k.sum()\n",
    "\n",
    "class CapsuleLoss(nn.Module):\n",
    "\tdef __init__(self, loss_lambda=0.5, recon_loss_scale=5e-4, size_average=False):\n",
    "\t\t'''\n",
    "\t\tCombined margin loss and reconstruction loss. Margin loss see above.\n",
    "\t\tSum squared error (SSE) was used as a reconstruction loss.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\trecon_loss_scale: \tparam for scaling down the the reconstruction loss\n",
    "\t\t\tsize_average:\t\tif True, reconstruction loss becomes MSE instead of SSE\n",
    "\t\t'''\n",
    "\t\tsuper(CapsuleLoss, self).__init__()\n",
    "\t\tself.size_average = size_average\n",
    "\t\tself.margin_loss = MarginLoss(size_average=size_average, loss_lambda=loss_lambda)\n",
    "\t\tself.reconstruction_loss = nn.MSELoss(size_average=size_average)\n",
    "\t\tself.recon_loss_scale = recon_loss_scale\n",
    "\n",
    "\tdef forward(self, inputs, labels, images, reconstructions):\n",
    "\t\tmargin_loss = self.margin_loss(inputs, labels)\n",
    "\t\treconstruction_loss = self.reconstruction_loss(reconstructions, images)\n",
    "\t\tcaps_loss = (margin_loss + self.recon_loss_scale * reconstruction_loss)\n",
    "\n",
    "\t\treturn caps_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Nm_b0_oAt3q9",
    "outputId": "5320eecb-7c47-4d3a-ba17-76c9cbaf19f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: cd: can't cd to drive\n",
      "/home/sunny/Documents/torchCaps\n"
     ]
    }
   ],
   "source": [
    "! cd drive\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDDvMYV-t51A"
   },
   "outputs": [],
   "source": [
    "SAVE_MODEL_PATH = 'checkpoints/'\n",
    "SAVE_IMG_PATH = 'outputs/'\n",
    "if not os.path.exists(SAVE_MODEL_PATH):\n",
    "    os.mkdir(SAVE_MODEL_PATH)\n",
    "if not os.path.exists(SAVE_IMG_PATH):\n",
    "    os.mkdir(SAVE_IMG_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IOcCgfx-yzHW"
   },
   "outputs": [],
   "source": [
    "class CapsNetTrainer:\n",
    "    \"\"\"\n",
    "    Wrapper object for handling training and evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loaders, batch_size, learning_rate, num_routing=3, lr_decay=0.9, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), multi_gpu=(torch.cuda.device_count() > 1)):\n",
    "        self.device = device\n",
    "        self.multi_gpu = multi_gpu\n",
    "\n",
    "        self.loaders = loaders\n",
    "        img_shape = self.loaders['train'].dataset[0][0].numpy().shape\n",
    "\n",
    "        self.net = CapsuleNetwork(img_shape=img_shape, channels=256, primary_dim=8, num_classes=10, out_dim=16, num_routing=num_routing, device=self.device).to(self.device)\n",
    "\n",
    "        if self.multi_gpu:\n",
    "            self.net = nn.DataParallel(self.net)\n",
    "\n",
    "        self.criterion = CapsuleLoss(loss_lambda=0.5, recon_loss_scale=5e-4)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=lr_decay)\n",
    "        print(8*'#', 'PyTorch Model built'.upper(), 8*'#')\n",
    "        print('Num params:', sum([prod(p.size()) for p in self.net.parameters()]))\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.net)\n",
    "\n",
    "    def run(self, epochs, classes):\n",
    "        SETPOINT = 2\n",
    "\n",
    "        print(8*'#', 'Run started'.upper(), 8*'#')\n",
    "        eye = torch.eye(len(classes)).to(self.device)\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "            for phase in ['train', 'test']:\n",
    "                print(f'{phase}ing...'.capitalize())\n",
    "                if phase == 'train':\n",
    "                    self.net.train()\n",
    "                else:\n",
    "                    self.net.eval()\n",
    "\n",
    "                t0 = time()\n",
    "                running_loss = 0.0\n",
    "                correct = 0; total = 0\n",
    "                for i, (images, labels) in enumerate(self.loaders[phase]):\n",
    "                    t1 = time()\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    # One-hot encode labels\n",
    "                    labels = eye[labels]\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    outputs, reconstructions = self.net(images)\n",
    "                    loss = self.criterion(outputs, labels, images, reconstructions)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        self.optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    _, labels = torch.max(labels, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum()\n",
    "                    accuracy = float(correct) / float(total)\n",
    "\n",
    "                    if phase == 'train' and epoch%SETPOINT == 0:\n",
    "                        print(f'Epoch {epoch}, Batch {i+1}, Loss {running_loss/(i+1)}',f'Accuracy {accuracy} Time {round(time()-t1, 3)}s')\n",
    "                        img = reconstructions[0].cpu().detach().numpy().reshape(28,28)\n",
    "                        plt.imshow(img)\n",
    "                        plt.savefig(os.path.join(SAVE_IMG_PATH,f'{predicted}.jpg'))\n",
    "\n",
    "\n",
    "                if epoch%1 == 0:\n",
    "                    print(f'{phase.upper()} Epoch {epoch}, Loss {running_loss/(i+1)}',f'Accuracy {accuracy} Time {round(time()-t0, 3)}s')\n",
    "\n",
    "\n",
    "            self.scheduler.step()\n",
    "\n",
    "        now = str(datetime.now()).replace(\" \", \"-\")\n",
    "        error_rate = round((1-accuracy)*100, 2)\n",
    "        torch.save(self.net.state_dict(), os.path.join(SAVE_MODEL_PATH, f'{error_rate}_{now}.pth.tar'))\n",
    "\n",
    "        class_correct = list(0. for _ in classes)\n",
    "        class_total = list(0. for _ in classes)\n",
    "        for images, labels in self.loaders['test']:\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "            outputs, reconstructions = self.net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "\n",
    "        for i in range(len(classes)):\n",
    "            print('Accuracy of %5s : %2d %%' % (\n",
    "                classes[i], 100 * class_correct[i] / class_total[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NlDkSCeJyhVp"
   },
   "outputs": [],
   "source": [
    "classes = list(range(10))\n",
    "mean, std = ( ( 0.1307,), ( 0.3081,) )\n",
    "size = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jXjPvSv8oGA_"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # shift by 2 pixels in either direction with zero padding.\n",
    "    transforms.RandomCrop(size, padding=2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize( mean, std )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "di5Ox4BwoSwz",
    "outputId": "04df1437-e59e-402a-e04c-d109d9f0caaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "==>>> total trainning batch number: 60000\n",
      "==>>> total testing batch number: 10000\n",
      "######## PYTORCH MODEL BUILT ########\n",
      "Num params: 8215568\n",
      "######## RUN STARTED ########\n",
      "Training...\n",
      "TRAIN Epoch 1, Loss 13.411377619934083 Accuracy 0.8981833333333333 Time 1005.738s\n",
      "Testing...\n",
      "TEST Epoch 1, Loss 10.347795210707302 Accuracy 0.9753 Time 45.51s\n",
      "Training...\n",
      "Epoch 2, Batch 1, Loss 11.774795532226562 Accuracy 0.90625 Time 0.501s\n",
      "Epoch 2, Batch 2, Loss 11.348848342895508 Accuracy 0.9375 Time 0.501s\n",
      "Epoch 2, Batch 3, Loss 11.063324928283691 Accuracy 0.9479166666666666 Time 0.514s\n",
      "Epoch 2, Batch 4, Loss 10.580819845199585 Accuracy 0.9609375 Time 0.517s\n",
      "Epoch 2, Batch 5, Loss 10.637977409362794 Accuracy 0.9625 Time 0.517s\n",
      "Epoch 2, Batch 6, Loss 10.435028711954752 Accuracy 0.9635416666666666 Time 0.518s\n",
      "Epoch 2, Batch 7, Loss 10.467030933925084 Accuracy 0.9642857142857143 Time 0.517s\n",
      "Epoch 2, Batch 8, Loss 10.39959728717804 Accuracy 0.96875 Time 0.521s\n",
      "Epoch 2, Batch 9, Loss 10.483274777730307 Accuracy 0.96875 Time 0.517s\n",
      "Epoch 2, Batch 10, Loss 10.462191677093506 Accuracy 0.971875 Time 0.515s\n",
      "Epoch 2, Batch 11, Loss 10.435113993558017 Accuracy 0.9744318181818182 Time 0.518s\n",
      "Epoch 2, Batch 12, Loss 10.322426557540894 Accuracy 0.9739583333333334 Time 0.515s\n",
      "Epoch 2, Batch 13, Loss 10.328834020174467 Accuracy 0.9759615384615384 Time 0.518s\n",
      "Epoch 2, Batch 14, Loss 10.404894351959229 Accuracy 0.9754464285714286 Time 0.515s\n",
      "Epoch 2, Batch 15, Loss 10.387874984741211 Accuracy 0.975 Time 0.514s\n",
      "Epoch 2, Batch 16, Loss 10.288147509098053 Accuracy 0.9765625 Time 0.515s\n",
      "Epoch 2, Batch 17, Loss 10.27839744792265 Accuracy 0.9779411764705882 Time 0.839s\n",
      "Epoch 2, Batch 18, Loss 10.347409725189209 Accuracy 0.9756944444444444 Time 0.523s\n",
      "Epoch 2, Batch 19, Loss 10.322170207374974 Accuracy 0.975328947368421 Time 0.514s\n",
      "Epoch 2, Batch 20, Loss 10.392331218719482 Accuracy 0.975 Time 0.517s\n",
      "Epoch 2, Batch 21, Loss 10.342483338855562 Accuracy 0.9761904761904762 Time 0.515s\n",
      "Epoch 2, Batch 22, Loss 10.26509480042891 Accuracy 0.9772727272727273 Time 0.52s\n",
      "Epoch 2, Batch 23, Loss 10.319930159527322 Accuracy 0.9741847826086957 Time 0.514s\n",
      "Epoch 2, Batch 24, Loss 10.371570070584616 Accuracy 0.97265625 Time 0.517s\n",
      "Epoch 2, Batch 25, Loss 10.394093627929687 Accuracy 0.97 Time 0.516s\n",
      "Epoch 2, Batch 26, Loss 10.408862334031324 Accuracy 0.9711538461538461 Time 0.517s\n",
      "Epoch 2, Batch 27, Loss 10.425238503350151 Accuracy 0.9722222222222222 Time 0.515s\n",
      "Epoch 2, Batch 28, Loss 10.396746499197823 Accuracy 0.9720982142857143 Time 0.516s\n",
      "Epoch 2, Batch 29, Loss 10.428653256646518 Accuracy 0.9719827586206896 Time 0.515s\n",
      "Epoch 2, Batch 30, Loss 10.415771357218425 Accuracy 0.9729166666666667 Time 0.515s\n",
      "Epoch 2, Batch 31, Loss 10.42210643522201 Accuracy 0.9717741935483871 Time 0.516s\n",
      "Epoch 2, Batch 32, Loss 10.43537700176239 Accuracy 0.96875 Time 0.517s\n",
      "Epoch 2, Batch 33, Loss 10.397279276992336 Accuracy 0.9696969696969697 Time 0.518s\n",
      "Epoch 2, Batch 34, Loss 10.3776935970082 Accuracy 0.9705882352941176 Time 0.515s\n",
      "Epoch 2, Batch 35, Loss 10.397733879089355 Accuracy 0.9696428571428571 Time 0.517s\n",
      "Epoch 2, Batch 36, Loss 10.409606986575657 Accuracy 0.96875 Time 0.517s\n",
      "Epoch 2, Batch 37, Loss 10.407327858177391 Accuracy 0.96875 Time 0.517s\n",
      "Epoch 2, Batch 38, Loss 10.432382332651239 Accuracy 0.9671052631578947 Time 0.515s\n",
      "Epoch 2, Batch 39, Loss 10.457824755937626 Accuracy 0.9655448717948718 Time 0.503s\n",
      "Epoch 2, Batch 40, Loss 10.452535271644592 Accuracy 0.965625 Time 0.505s\n",
      "Epoch 2, Batch 41, Loss 10.430664411405237 Accuracy 0.9664634146341463 Time 0.506s\n",
      "Epoch 2, Batch 42, Loss 10.41506814956665 Accuracy 0.9672619047619048 Time 0.501s\n",
      "Epoch 2, Batch 43, Loss 10.417419943698617 Accuracy 0.967296511627907 Time 0.5s\n",
      "Epoch 2, Batch 44, Loss 10.419757864691995 Accuracy 0.9673295454545454 Time 0.501s\n",
      "Epoch 2, Batch 45, Loss 10.400081295437284 Accuracy 0.9680555555555556 Time 0.501s\n",
      "Epoch 2, Batch 46, Loss 10.409624804621158 Accuracy 0.96875 Time 0.501s\n",
      "Epoch 2, Batch 47, Loss 10.417067791553254 Accuracy 0.9674202127659575 Time 0.508s\n",
      "Epoch 2, Batch 48, Loss 10.435625831286112 Accuracy 0.966796875 Time 0.501s\n",
      "Epoch 2, Batch 49, Loss 10.449636420425104 Accuracy 0.9674744897959183 Time 0.501s\n",
      "Epoch 2, Batch 50, Loss 10.420948085784913 Accuracy 0.968125 Time 0.501s\n",
      "Epoch 2, Batch 51, Loss 10.41379317115335 Accuracy 0.9681372549019608 Time 0.5s\n",
      "Epoch 2, Batch 52, Loss 10.41666291310237 Accuracy 0.9681490384615384 Time 0.501s\n",
      "Epoch 2, Batch 53, Loss 10.436488601396668 Accuracy 0.9663915094339622 Time 0.501s\n",
      "Epoch 2, Batch 54, Loss 10.502657236876312 Accuracy 0.9652777777777778 Time 0.501s\n",
      "Epoch 2, Batch 55, Loss 10.492156462235885 Accuracy 0.9659090909090909 Time 0.5s\n",
      "Epoch 2, Batch 56, Loss 10.496073143822807 Accuracy 0.96484375 Time 0.501s\n",
      "Epoch 2, Batch 57, Loss 10.498039730808191 Accuracy 0.9649122807017544 Time 0.501s\n",
      "Epoch 2, Batch 58, Loss 10.504965667066903 Accuracy 0.9644396551724138 Time 0.501s\n",
      "Epoch 2, Batch 59, Loss 10.528558036028329 Accuracy 0.9639830508474576 Time 0.5s\n",
      "Epoch 2, Batch 60, Loss 10.52126054763794 Accuracy 0.9640625 Time 0.501s\n",
      "Epoch 2, Batch 61, Loss 10.521853634568512 Accuracy 0.9641393442622951 Time 0.501s\n",
      "Epoch 2, Batch 62, Loss 10.517624393586189 Accuracy 0.9642137096774194 Time 0.501s\n",
      "Epoch 2, Batch 63, Loss 10.517403405810159 Accuracy 0.9637896825396826 Time 0.5s\n",
      "Epoch 2, Batch 64, Loss 10.52194507420063 Accuracy 0.9638671875 Time 0.501s\n",
      "Epoch 2, Batch 65, Loss 10.534377391521748 Accuracy 0.9629807692307693 Time 0.501s\n",
      "Epoch 2, Batch 66, Loss 10.532355337431937 Accuracy 0.9635416666666666 Time 0.501s\n",
      "Epoch 2, Batch 67, Loss 10.52868626722649 Accuracy 0.9636194029850746 Time 0.5s\n",
      "Epoch 2, Batch 68, Loss 10.519436050863828 Accuracy 0.9636948529411765 Time 0.501s\n",
      "Epoch 2, Batch 69, Loss 10.497585835664168 Accuracy 0.9642210144927537 Time 0.501s\n",
      "Epoch 2, Batch 70, Loss 10.487962300436838 Accuracy 0.9638392857142857 Time 0.501s\n",
      "Epoch 2, Batch 71, Loss 10.495933062593702 Accuracy 0.9639084507042254 Time 0.5s\n",
      "Epoch 2, Batch 72, Loss 10.490202678574455 Accuracy 0.9644097222222222 Time 0.501s\n",
      "Epoch 2, Batch 73, Loss 10.490804371768482 Accuracy 0.9648972602739726 Time 0.501s\n",
      "Epoch 2, Batch 74, Loss 10.474212092322272 Accuracy 0.9653716216216216 Time 0.501s\n",
      "Epoch 2, Batch 75, Loss 10.476085815429688 Accuracy 0.9645833333333333 Time 0.5s\n",
      "Epoch 2, Batch 76, Loss 10.466252741060758 Accuracy 0.9642269736842105 Time 0.501s\n",
      "Epoch 2, Batch 77, Loss 10.46161022433987 Accuracy 0.9638798701298701 Time 0.501s\n",
      "Epoch 2, Batch 78, Loss 10.451039277590239 Accuracy 0.9643429487179487 Time 0.501s\n",
      "Epoch 2, Batch 79, Loss 10.455790036841284 Accuracy 0.9636075949367089 Time 0.5s\n",
      "Epoch 2, Batch 80, Loss 10.46094000339508 Accuracy 0.963671875 Time 0.501s\n",
      "Epoch 2, Batch 81, Loss 10.457355193149896 Accuracy 0.9633487654320988 Time 0.501s\n",
      "Epoch 2, Batch 82, Loss 10.441100841615258 Accuracy 0.963795731707317 Time 0.501s\n",
      "Epoch 2, Batch 83, Loss 10.45864544144596 Accuracy 0.9627259036144579 Time 0.507s\n",
      "Epoch 2, Batch 84, Loss 10.451432387034098 Accuracy 0.9631696428571429 Time 0.501s\n",
      "Epoch 2, Batch 85, Loss 10.43876838684082 Accuracy 0.9636029411764706 Time 0.501s\n",
      "Epoch 2, Batch 86, Loss 10.43371201670447 Accuracy 0.9636627906976745 Time 0.501s\n",
      "Epoch 2, Batch 87, Loss 10.430363435854858 Accuracy 0.9633620689655172 Time 0.5s\n",
      "Epoch 2, Batch 88, Loss 10.407355947927995 Accuracy 0.9637784090909091 Time 0.501s\n",
      "Epoch 2, Batch 89, Loss 10.410443338115563 Accuracy 0.9634831460674157 Time 0.501s\n",
      "Epoch 2, Batch 90, Loss 10.40705738067627 Accuracy 0.9635416666666666 Time 0.501s\n",
      "Epoch 2, Batch 91, Loss 10.406845480531127 Accuracy 0.9635989010989011 Time 0.5s\n",
      "Epoch 2, Batch 92, Loss 10.409401903981747 Accuracy 0.9636548913043478 Time 0.501s\n",
      "Epoch 2, Batch 93, Loss 10.399366912021431 Accuracy 0.9640456989247311 Time 0.501s\n",
      "Epoch 2, Batch 94, Loss 10.397650698398023 Accuracy 0.9634308510638298 Time 0.501s\n",
      "Epoch 2, Batch 95, Loss 10.389901954249332 Accuracy 0.9638157894736842 Time 0.5s\n",
      "Epoch 2, Batch 96, Loss 10.383386760950089 Accuracy 0.9635416666666666 Time 0.501s\n",
      "Epoch 2, Batch 97, Loss 10.37519823644579 Accuracy 0.9639175257731959 Time 0.501s\n",
      "Epoch 2, Batch 98, Loss 10.37766068322318 Accuracy 0.9642857142857143 Time 0.501s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 99, Loss 10.370744512538717 Accuracy 0.9646464646464646 Time 0.5s\n",
      "Epoch 2, Batch 100, Loss 10.354500913619995 Accuracy 0.965 Time 0.501s\n",
      "Epoch 2, Batch 101, Loss 10.33928909868297 Accuracy 0.9653465346534653 Time 0.604s\n",
      "Epoch 2, Batch 102, Loss 10.325846784255084 Accuracy 0.9653799019607843 Time 0.501s\n",
      "Epoch 2, Batch 103, Loss 10.320318370189481 Accuracy 0.9657160194174758 Time 0.5s\n",
      "Epoch 2, Batch 104, Loss 10.322299131980309 Accuracy 0.9654447115384616 Time 0.501s\n",
      "Epoch 2, Batch 105, Loss 10.319713801429385 Accuracy 0.9654761904761905 Time 0.501s\n",
      "Epoch 2, Batch 106, Loss 10.314724355373743 Accuracy 0.9655070754716981 Time 0.501s\n",
      "Epoch 2, Batch 107, Loss 10.299098915028795 Accuracy 0.9658294392523364 Time 0.5s\n",
      "Epoch 2, Batch 108, Loss 10.292126364178127 Accuracy 0.9658564814814815 Time 0.502s\n",
      "Epoch 2, Batch 109, Loss 10.287038496874889 Accuracy 0.9658830275229358 Time 0.501s\n",
      "Epoch 2, Batch 110, Loss 10.279076021367853 Accuracy 0.9661931818181818 Time 0.501s\n",
      "Epoch 2, Batch 111, Loss 10.270916741173547 Accuracy 0.9664977477477478 Time 0.5s\n",
      "Epoch 2, Batch 112, Loss 10.271386333874293 Accuracy 0.966796875 Time 0.501s\n",
      "Epoch 2, Batch 113, Loss 10.26183127512974 Accuracy 0.9670907079646017 Time 0.501s\n",
      "Epoch 2, Batch 114, Loss 10.264634291330973 Accuracy 0.9665570175438597 Time 0.501s\n",
      "Epoch 2, Batch 115, Loss 10.253340323075005 Accuracy 0.9668478260869565 Time 0.5s\n",
      "Epoch 2, Batch 116, Loss 10.251299389477435 Accuracy 0.9671336206896551 Time 0.501s\n",
      "Epoch 2, Batch 117, Loss 10.242476308447683 Accuracy 0.9671474358974359 Time 0.501s\n",
      "Epoch 2, Batch 118, Loss 10.237173759331137 Accuracy 0.966896186440678 Time 0.501s\n",
      "Epoch 2, Batch 119, Loss 10.235266757612468 Accuracy 0.9671743697478992 Time 0.5s\n",
      "Epoch 2, Batch 120, Loss 10.229988082249958 Accuracy 0.9671875 Time 0.501s\n",
      "Epoch 2, Batch 121, Loss 10.22638341415027 Accuracy 0.9674586776859504 Time 0.501s\n",
      "Epoch 2, Batch 122, Loss 10.219494350620957 Accuracy 0.967469262295082 Time 0.501s\n",
      "Epoch 2, Batch 123, Loss 10.206106403009679 Accuracy 0.9677337398373984 Time 0.5s\n",
      "Epoch 2, Batch 124, Loss 10.198427646390853 Accuracy 0.9679939516129032 Time 0.501s\n",
      "Epoch 2, Batch 125, Loss 10.193472724914551 Accuracy 0.96825 Time 0.501s\n",
      "Epoch 2, Batch 126, Loss 10.177400838761102 Accuracy 0.9685019841269841 Time 0.501s\n",
      "Epoch 2, Batch 127, Loss 10.178711403073288 Accuracy 0.968503937007874 Time 0.5s\n",
      "Epoch 2, Batch 128, Loss 10.16950636357069 Accuracy 0.96875 Time 0.501s\n",
      "Epoch 2, Batch 129, Loss 10.158864176550576 Accuracy 0.96875 Time 0.501s\n",
      "Epoch 2, Batch 130, Loss 10.14795168363131 Accuracy 0.9689903846153847 Time 0.501s\n",
      "Epoch 2, Batch 131, Loss 10.148548402859054 Accuracy 0.9692270992366412 Time 0.524s\n",
      "Epoch 2, Batch 132, Loss 10.144379095597701 Accuracy 0.9694602272727273 Time 0.501s\n",
      "Epoch 2, Batch 133, Loss 10.137102815441619 Accuracy 0.9694548872180451 Time 0.5s\n",
      "Epoch 2, Batch 134, Loss 10.131690687208033 Accuracy 0.9694496268656716 Time 0.543s\n",
      "Epoch 2, Batch 135, Loss 10.132413977163809 Accuracy 0.9694444444444444 Time 0.5s\n",
      "Epoch 2, Batch 136, Loss 10.12887435800889 Accuracy 0.9694393382352942 Time 0.501s\n",
      "Epoch 2, Batch 137, Loss 10.119420705920588 Accuracy 0.969434306569343 Time 0.501s\n",
      "Epoch 2, Batch 138, Loss 10.114427013673644 Accuracy 0.9696557971014492 Time 0.501s\n",
      "Epoch 2, Batch 139, Loss 10.106043863639558 Accuracy 0.9698741007194245 Time 0.5s\n",
      "Epoch 2, Batch 140, Loss 10.105792093276978 Accuracy 0.9698660714285714 Time 0.501s\n",
      "Epoch 2, Batch 141, Loss 10.109039137549434 Accuracy 0.969636524822695 Time 0.501s\n",
      "Epoch 2, Batch 142, Loss 10.108069842969867 Accuracy 0.9696302816901409 Time 0.501s\n",
      "Epoch 2, Batch 143, Loss 10.098239025035939 Accuracy 0.9698426573426573 Time 0.5s\n",
      "Epoch 2, Batch 144, Loss 10.096628798378838 Accuracy 0.9698350694444444 Time 0.501s\n",
      "Epoch 2, Batch 145, Loss 10.095294432804502 Accuracy 0.9700431034482758 Time 0.502s\n",
      "Epoch 2, Batch 146, Loss 10.088104032490351 Accuracy 0.969820205479452 Time 0.501s\n",
      "Epoch 2, Batch 147, Loss 10.082505978694579 Accuracy 0.9700255102040817 Time 0.501s\n",
      "Epoch 2, Batch 148, Loss 10.083141513772913 Accuracy 0.9698057432432432 Time 0.501s\n",
      "Epoch 2, Batch 149, Loss 10.074112796143398 Accuracy 0.9697986577181208 Time 0.501s\n",
      "Epoch 2, Batch 150, Loss 10.066038837432862 Accuracy 0.9697916666666667 Time 0.501s\n",
      "Epoch 2, Batch 151, Loss 10.066121164536634 Accuracy 0.9699917218543046 Time 0.5s\n",
      "Epoch 2, Batch 152, Loss 10.069180632892408 Accuracy 0.9701891447368421 Time 0.501s\n",
      "Epoch 2, Batch 153, Loss 10.064399650673462 Accuracy 0.9701797385620915 Time 0.501s\n",
      "Epoch 2, Batch 154, Loss 10.05964402409343 Accuracy 0.9699675324675324 Time 0.501s\n",
      "Epoch 2, Batch 155, Loss 10.060833075738723 Accuracy 0.9701612903225807 Time 0.5s\n",
      "Epoch 2, Batch 156, Loss 10.057636309892704 Accuracy 0.9703525641025641 Time 0.501s\n",
      "Epoch 2, Batch 157, Loss 10.060622397501756 Accuracy 0.9703423566878981 Time 0.501s\n",
      "Epoch 2, Batch 158, Loss 10.06782773174817 Accuracy 0.9701344936708861 Time 0.501s\n",
      "Epoch 2, Batch 159, Loss 10.063843721113864 Accuracy 0.970125786163522 Time 0.5s\n",
      "Epoch 2, Batch 160, Loss 10.066213965415955 Accuracy 0.9703125 Time 0.501s\n",
      "Epoch 2, Batch 161, Loss 10.066106310542326 Accuracy 0.9704968944099379 Time 0.501s\n",
      "Epoch 2, Batch 162, Loss 10.070755046090962 Accuracy 0.9704861111111112 Time 0.501s\n",
      "Epoch 2, Batch 163, Loss 10.068814371261128 Accuracy 0.9706671779141104 Time 0.5s\n",
      "Epoch 2, Batch 164, Loss 10.07704690026074 Accuracy 0.9702743902439024 Time 0.502s\n",
      "Epoch 2, Batch 165, Loss 10.076500696124452 Accuracy 0.9704545454545455 Time 0.501s\n",
      "Epoch 2, Batch 166, Loss 10.07907665781228 Accuracy 0.9706325301204819 Time 0.501s\n",
      "Epoch 2, Batch 167, Loss 10.081569763000854 Accuracy 0.9706212574850299 Time 0.5s\n",
      "Epoch 2, Batch 168, Loss 10.083724748520623 Accuracy 0.9706101190476191 Time 0.501s\n",
      "Epoch 2, Batch 169, Loss 10.089487459532608 Accuracy 0.9705991124260355 Time 0.501s\n",
      "Epoch 2, Batch 170, Loss 10.09536771213307 Accuracy 0.9704044117647059 Time 0.501s\n",
      "Epoch 2, Batch 171, Loss 10.094733243797258 Accuracy 0.9702119883040936 Time 0.5s\n",
      "Epoch 2, Batch 172, Loss 10.093703929768052 Accuracy 0.9703851744186046 Time 0.501s\n",
      "Epoch 2, Batch 173, Loss 10.094398404821495 Accuracy 0.9703757225433526 Time 0.501s\n",
      "Epoch 2, Batch 174, Loss 10.090111272088413 Accuracy 0.9705459770114943 Time 0.501s\n",
      "Epoch 2, Batch 175, Loss 10.089225224086217 Accuracy 0.9705357142857143 Time 0.5s\n",
      "Epoch 2, Batch 176, Loss 10.091805604371158 Accuracy 0.9703480113636364 Time 0.501s\n",
      "Epoch 2, Batch 177, Loss 10.091276346626929 Accuracy 0.9703389830508474 Time 0.503s\n",
      "Epoch 2, Batch 178, Loss 10.088578561718544 Accuracy 0.9705056179775281 Time 0.501s\n",
      "Epoch 2, Batch 179, Loss 10.088567494014121 Accuracy 0.9704958100558659 Time 0.5s\n",
      "Epoch 2, Batch 180, Loss 10.08891512023078 Accuracy 0.9706597222222222 Time 0.501s\n",
      "Epoch 2, Batch 181, Loss 10.094636911845338 Accuracy 0.9706491712707183 Time 0.501s\n",
      "Epoch 2, Batch 182, Loss 10.100094548948519 Accuracy 0.970467032967033 Time 0.5s\n",
      "Epoch 2, Batch 183, Loss 10.09998175094688 Accuracy 0.9702868852459017 Time 0.5s\n",
      "Epoch 2, Batch 184, Loss 10.107797249503758 Accuracy 0.970108695652174 Time 0.501s\n",
      "Epoch 2, Batch 185, Loss 10.105664665634567 Accuracy 0.9702702702702702 Time 0.501s\n",
      "Epoch 2, Batch 186, Loss 10.104063280167118 Accuracy 0.9704301075268817 Time 0.501s\n",
      "Epoch 2, Batch 187, Loss 10.105891783607197 Accuracy 0.9704211229946524 Time 0.5s\n",
      "Epoch 2, Batch 188, Loss 10.103036423946948 Accuracy 0.9705784574468085 Time 0.501s\n",
      "Epoch 2, Batch 189, Loss 10.107747552256106 Accuracy 0.9704034391534392 Time 0.501s\n",
      "Epoch 2, Batch 190, Loss 10.10822477842632 Accuracy 0.9705592105263158 Time 0.501s\n",
      "Epoch 2, Batch 191, Loss 10.111604990135312 Accuracy 0.9707133507853403 Time 0.501s\n",
      "Epoch 2, Batch 192, Loss 10.109083702166876 Accuracy 0.9708658854166666 Time 0.501s\n",
      "Epoch 2, Batch 193, Loss 10.11771415552327 Accuracy 0.9705310880829016 Time 0.501s\n",
      "Epoch 2, Batch 194, Loss 10.114565033273598 Accuracy 0.9706829896907216 Time 0.501s\n",
      "Epoch 2, Batch 195, Loss 10.111122679099058 Accuracy 0.9708333333333333 Time 0.5s\n",
      "Epoch 2, Batch 196, Loss 10.111790978178686 Accuracy 0.9708227040816326 Time 0.501s\n",
      "Epoch 2, Batch 197, Loss 10.106536744209716 Accuracy 0.9709708121827412 Time 0.501s\n",
      "Epoch 2, Batch 198, Loss 10.106141682827111 Accuracy 0.9709595959595959 Time 0.501s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 199, Loss 10.105267534303906 Accuracy 0.9709484924623115 Time 0.5s\n",
      "Epoch 2, Batch 200, Loss 10.103788280487061 Accuracy 0.97109375 Time 0.501s\n",
      "Epoch 2, Batch 201, Loss 10.102427468371035 Accuracy 0.9712375621890548 Time 0.501s\n",
      "Epoch 2, Batch 202, Loss 10.103063347316024 Accuracy 0.9712252475247525 Time 0.503s\n",
      "Epoch 2, Batch 203, Loss 10.114690620910945 Accuracy 0.9710591133004927 Time 0.5s\n",
      "Epoch 2, Batch 204, Loss 10.112863110560998 Accuracy 0.9710477941176471 Time 0.501s\n",
      "Epoch 2, Batch 205, Loss 10.110963198033774 Accuracy 0.9711890243902439 Time 0.501s\n",
      "Epoch 2, Batch 206, Loss 10.107490150673875 Accuracy 0.9713288834951457 Time 0.501s\n",
      "Epoch 2, Batch 207, Loss 10.110414081149631 Accuracy 0.971316425120773 Time 0.5s\n",
      "Epoch 2, Batch 208, Loss 10.108476570019356 Accuracy 0.9713040865384616 Time 0.501s\n",
      "Epoch 2, Batch 209, Loss 10.103786628212085 Accuracy 0.9714413875598086 Time 0.501s\n",
      "Epoch 2, Batch 210, Loss 10.103596137818837 Accuracy 0.971279761904762 Time 0.501s\n",
      "Epoch 2, Batch 211, Loss 10.10179564399177 Accuracy 0.9709715639810427 Time 0.5s\n",
      "Epoch 2, Batch 212, Loss 10.100542045989126 Accuracy 0.9709610849056604 Time 0.501s\n",
      "Epoch 2, Batch 213, Loss 10.104634858073203 Accuracy 0.9709507042253521 Time 0.501s\n",
      "Epoch 2, Batch 214, Loss 10.104181739771478 Accuracy 0.9710864485981309 Time 0.501s\n",
      "Epoch 2, Batch 215, Loss 10.105175856656807 Accuracy 0.9710755813953489 Time 0.5s\n",
      "Epoch 2, Batch 216, Loss 10.106469300058153 Accuracy 0.9710648148148148 Time 0.501s\n",
      "Epoch 2, Batch 217, Loss 10.101866414470058 Accuracy 0.9710541474654378 Time 0.501s\n",
      "Epoch 2, Batch 218, Loss 10.099905429630105 Accuracy 0.9711869266055045 Time 0.501s\n",
      "Epoch 2, Batch 219, Loss 10.10677730002904 Accuracy 0.9708904109589042 Time 0.5s\n",
      "Epoch 2, Batch 220, Loss 10.10924036286094 Accuracy 0.9707386363636363 Time 0.501s\n",
      "Epoch 2, Batch 221, Loss 10.108035199782428 Accuracy 0.9708710407239819 Time 0.501s\n",
      "Epoch 2, Batch 222, Loss 10.103473001772219 Accuracy 0.9710022522522522 Time 0.501s\n",
      "Epoch 2, Batch 223, Loss 10.107648776785675 Accuracy 0.9707118834080718 Time 0.5s\n",
      "Epoch 2, Batch 224, Loss 10.106127215283257 Accuracy 0.970703125 Time 0.501s\n",
      "Epoch 2, Batch 225, Loss 10.101444384256999 Accuracy 0.9708333333333333 Time 0.501s\n",
      "Epoch 2, Batch 226, Loss 10.098817242985278 Accuracy 0.9709623893805309 Time 0.501s\n",
      "Epoch 2, Batch 227, Loss 10.098241511945682 Accuracy 0.9709526431718062 Time 0.549s\n",
      "Epoch 2, Batch 228, Loss 10.099010777055172 Accuracy 0.9709429824561403 Time 0.501s\n",
      "Epoch 2, Batch 229, Loss 10.093577597339081 Accuracy 0.9710698689956332 Time 0.501s\n",
      "Epoch 2, Batch 230, Loss 10.094530532671058 Accuracy 0.9710597826086956 Time 0.501s\n",
      "Epoch 2, Batch 231, Loss 10.096865237017214 Accuracy 0.9709145021645021 Time 0.5s\n",
      "Epoch 2, Batch 232, Loss 10.09688313665061 Accuracy 0.970770474137931 Time 0.501s\n",
      "Epoch 2, Batch 233, Loss 10.099614454441316 Accuracy 0.9707618025751072 Time 0.501s\n",
      "Epoch 2, Batch 234, Loss 10.094629291795258 Accuracy 0.9707532051282052 Time 0.501s\n",
      "Epoch 2, Batch 235, Loss 10.094343721105698 Accuracy 0.970877659574468 Time 0.5s\n",
      "Epoch 2, Batch 236, Loss 10.08700643959692 Accuracy 0.9708686440677966 Time 0.501s\n",
      "Epoch 2, Batch 237, Loss 10.079990994578173 Accuracy 0.9709915611814346 Time 0.501s\n",
      "Epoch 2, Batch 238, Loss 10.084075767452978 Accuracy 0.9708508403361344 Time 0.501s\n",
      "Epoch 2, Batch 239, Loss 10.083869598899426 Accuracy 0.970842050209205 Time 0.5s\n",
      "Epoch 2, Batch 240, Loss 10.080163304011027 Accuracy 0.9708333333333333 Time 0.501s\n",
      "Epoch 2, Batch 241, Loss 10.077754400577783 Accuracy 0.970954356846473 Time 0.501s\n",
      "Epoch 2, Batch 242, Loss 10.079366175596379 Accuracy 0.9708161157024794 Time 0.501s\n",
      "Epoch 2, Batch 243, Loss 10.08229603787018 Accuracy 0.970679012345679 Time 0.5s\n",
      "Epoch 2, Batch 244, Loss 10.083238374991495 Accuracy 0.9706711065573771 Time 0.501s\n",
      "Epoch 2, Batch 245, Loss 10.080903185630332 Accuracy 0.9706632653061225 Time 0.501s\n",
      "Epoch 2, Batch 246, Loss 10.08097719176998 Accuracy 0.9706554878048781 Time 0.501s\n",
      "Epoch 2, Batch 247, Loss 10.078636737004947 Accuracy 0.9707742914979757 Time 0.5s\n",
      "Epoch 2, Batch 248, Loss 10.076852936898508 Accuracy 0.9708921370967742 Time 0.501s\n",
      "Epoch 2, Batch 249, Loss 10.076185444751419 Accuracy 0.9708835341365462 Time 0.501s\n",
      "Epoch 2, Batch 250, Loss 10.072453514099122 Accuracy 0.970875 Time 0.501s\n",
      "Epoch 2, Batch 251, Loss 10.073040361898354 Accuracy 0.9709910358565738 Time 0.5s\n",
      "Epoch 2, Batch 252, Loss 10.074971119562784 Accuracy 0.9709821428571429 Time 0.501s\n",
      "Epoch 2, Batch 253, Loss 10.072211714130145 Accuracy 0.9709733201581028 Time 0.501s\n",
      "Epoch 2, Batch 254, Loss 10.072731521185927 Accuracy 0.9709645669291339 Time 0.501s\n",
      "Epoch 2, Batch 255, Loss 10.072850358252431 Accuracy 0.9709558823529412 Time 0.5s\n",
      "Epoch 2, Batch 256, Loss 10.070545922964811 Accuracy 0.970947265625 Time 0.501s\n",
      "Epoch 2, Batch 257, Loss 10.068139050257345 Accuracy 0.9710603112840467 Time 0.501s\n",
      "Epoch 2, Batch 258, Loss 10.063286444937535 Accuracy 0.971172480620155 Time 0.501s\n",
      "Epoch 2, Batch 259, Loss 10.059460787239223 Accuracy 0.9712837837837838 Time 0.5s\n",
      "Epoch 2, Batch 260, Loss 10.061819923841036 Accuracy 0.9712740384615385 Time 0.501s\n",
      "Epoch 2, Batch 261, Loss 10.062707663495878 Accuracy 0.9712643678160919 Time 0.501s\n",
      "Epoch 2, Batch 262, Loss 10.061639156050354 Accuracy 0.9712547709923665 Time 0.501s\n",
      "Epoch 2, Batch 263, Loss 10.066604831826098 Accuracy 0.9710076045627376 Time 0.5s\n",
      "Epoch 2, Batch 264, Loss 10.062252510677684 Accuracy 0.9711174242424242 Time 0.501s\n",
      "Epoch 2, Batch 265, Loss 10.059980860296285 Accuracy 0.9712264150943396 Time 0.501s\n",
      "Epoch 2, Batch 266, Loss 10.057085259516436 Accuracy 0.9713345864661654 Time 0.501s\n",
      "Epoch 2, Batch 267, Loss 10.05974463130651 Accuracy 0.9713249063670412 Time 0.501s\n",
      "Epoch 2, Batch 268, Loss 10.058226670791854 Accuracy 0.9713152985074627 Time 0.501s\n",
      "Epoch 2, Batch 269, Loss 10.05488224454972 Accuracy 0.9714219330855018 Time 0.501s\n",
      "Epoch 2, Batch 270, Loss 10.049661247818559 Accuracy 0.9715277777777778 Time 0.501s\n",
      "Epoch 2, Batch 271, Loss 10.050614884858641 Accuracy 0.9714022140221402 Time 0.5s\n",
      "Epoch 2, Batch 272, Loss 10.051691917812123 Accuracy 0.9712775735294118 Time 0.501s\n",
      "Epoch 2, Batch 273, Loss 10.049853328383449 Accuracy 0.9713827838827839 Time 0.501s\n",
      "Epoch 2, Batch 274, Loss 10.047992963860505 Accuracy 0.9714872262773723 Time 0.501s\n",
      "Epoch 2, Batch 275, Loss 10.046856044422496 Accuracy 0.9715909090909091 Time 0.5s\n",
      "Epoch 2, Batch 276, Loss 10.045271306798078 Accuracy 0.9716938405797102 Time 0.501s\n",
      "Epoch 2, Batch 277, Loss 10.04183222842991 Accuracy 0.9717960288808665 Time 0.5s\n",
      "Epoch 2, Batch 278, Loss 10.043077036631193 Accuracy 0.9718974820143885 Time 0.501s\n",
      "Epoch 2, Batch 279, Loss 10.043384784438704 Accuracy 0.9719982078853047 Time 0.5s\n",
      "Epoch 2, Batch 280, Loss 10.03925347668784 Accuracy 0.9720982142857143 Time 0.501s\n",
      "Epoch 2, Batch 281, Loss 10.040257755971888 Accuracy 0.9717526690391459 Time 0.501s\n",
      "Epoch 2, Batch 282, Loss 10.044904654753124 Accuracy 0.971520390070922 Time 0.501s\n",
      "Epoch 2, Batch 283, Loss 10.044372123880017 Accuracy 0.9715106007067138 Time 0.5s\n",
      "Epoch 2, Batch 284, Loss 10.042415068183146 Accuracy 0.9713908450704225 Time 0.501s\n",
      "Epoch 2, Batch 285, Loss 10.043533847206517 Accuracy 0.9714912280701754 Time 0.501s\n",
      "Epoch 2, Batch 286, Loss 10.046606223900001 Accuracy 0.9713723776223776 Time 0.501s\n",
      "Epoch 2, Batch 287, Loss 10.043814848524352 Accuracy 0.9714721254355401 Time 0.5s\n",
      "Epoch 2, Batch 288, Loss 10.041156470775604 Accuracy 0.9714626736111112 Time 0.501s\n",
      "Epoch 2, Batch 289, Loss 10.042486923376169 Accuracy 0.9714532871972318 Time 0.5s\n",
      "Epoch 2, Batch 290, Loss 10.043323332687905 Accuracy 0.971551724137931 Time 0.501s\n",
      "Epoch 2, Batch 291, Loss 10.041185801791162 Accuracy 0.9716494845360825 Time 0.5s\n",
      "Epoch 2, Batch 292, Loss 10.039049269401865 Accuracy 0.9717465753424658 Time 0.501s\n",
      "Epoch 2, Batch 293, Loss 10.040304740541217 Accuracy 0.9717363481228669 Time 0.501s\n",
      "Epoch 2, Batch 294, Loss 10.041883578916796 Accuracy 0.9717261904761905 Time 0.501s\n",
      "Epoch 2, Batch 295, Loss 10.039442207853673 Accuracy 0.9718220338983051 Time 0.5s\n",
      "Epoch 2, Batch 296, Loss 10.039605833388663 Accuracy 0.9718116554054054 Time 0.501s\n",
      "Epoch 2, Batch 297, Loss 10.044981442717992 Accuracy 0.9716961279461279 Time 0.501s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 298, Loss 10.043068351361576 Accuracy 0.9716862416107382 Time 0.501s\n",
      "Epoch 2, Batch 299, Loss 10.045027471306332 Accuracy 0.9717809364548495 Time 0.5s\n",
      "Epoch 2, Batch 300, Loss 10.042664066950481 Accuracy 0.971875 Time 0.502s\n",
      "Epoch 2, Batch 301, Loss 10.038897980091184 Accuracy 0.971968438538206 Time 0.501s\n",
      "Epoch 2, Batch 302, Loss 10.038510489937485 Accuracy 0.9720612582781457 Time 0.5s\n",
      "Epoch 2, Batch 303, Loss 10.03774098437218 Accuracy 0.9718440594059405 Time 0.5s\n",
      "Epoch 2, Batch 304, Loss 10.039504327272114 Accuracy 0.9716282894736842 Time 0.501s\n",
      "Epoch 2, Batch 305, Loss 10.039869802506244 Accuracy 0.9717213114754099 Time 0.501s\n",
      "Epoch 2, Batch 306, Loss 10.038764401978137 Accuracy 0.9715073529411765 Time 0.501s\n",
      "Epoch 2, Batch 307, Loss 10.034983765418833 Accuracy 0.9716001628664495 Time 0.5s\n",
      "Epoch 2, Batch 308, Loss 10.035944669277637 Accuracy 0.9715909090909091 Time 0.501s\n",
      "Epoch 2, Batch 309, Loss 10.034251654418155 Accuracy 0.9716828478964401 Time 0.501s\n",
      "Epoch 2, Batch 310, Loss 10.030820480469734 Accuracy 0.9717741935483871 Time 0.501s\n",
      "Epoch 2, Batch 311, Loss 10.02772649844743 Accuracy 0.9718649517684887 Time 0.896s\n",
      "Epoch 2, Batch 312, Loss 10.022515679017092 Accuracy 0.9719551282051282 Time 0.55s\n",
      "Epoch 2, Batch 313, Loss 10.026184865080129 Accuracy 0.9718450479233227 Time 0.686s\n",
      "Epoch 2, Batch 314, Loss 10.02765183539907 Accuracy 0.9719347133757962 Time 0.554s\n",
      "Epoch 2, Batch 315, Loss 10.029263033185686 Accuracy 0.9718253968253968 Time 0.762s\n",
      "Epoch 2, Batch 316, Loss 10.028399177744419 Accuracy 0.9719145569620253 Time 0.512s\n",
      "Epoch 2, Batch 317, Loss 10.028216581615364 Accuracy 0.9719045741324921 Time 0.73s\n",
      "Epoch 2, Batch 318, Loss 10.029622269876349 Accuracy 0.9717963836477987 Time 0.672s\n",
      "Epoch 2, Batch 319, Loss 10.02889956650689 Accuracy 0.9715909090909091 Time 0.582s\n",
      "Epoch 2, Batch 320, Loss 10.026081374287605 Accuracy 0.97158203125 Time 0.746s\n",
      "Epoch 2, Batch 321, Loss 10.022820956981814 Accuracy 0.9716705607476636 Time 0.67s\n",
      "Epoch 2, Batch 322, Loss 10.01986312866211 Accuracy 0.9717585403726708 Time 0.677s\n",
      "Epoch 2, Batch 323, Loss 10.022064347754322 Accuracy 0.9718459752321982 Time 0.5s\n",
      "Epoch 2, Batch 324, Loss 10.018284482720457 Accuracy 0.9718364197530864 Time 0.501s\n",
      "Epoch 2, Batch 325, Loss 10.01773894383357 Accuracy 0.9719230769230769 Time 0.501s\n",
      "Epoch 2, Batch 326, Loss 10.018969986336362 Accuracy 0.9719133435582822 Time 0.501s\n",
      "Epoch 2, Batch 327, Loss 10.01694380325644 Accuracy 0.9719992354740061 Time 0.5s\n",
      "Epoch 2, Batch 328, Loss 10.018254742389772 Accuracy 0.9718940548780488 Time 0.501s\n",
      "Epoch 2, Batch 329, Loss 10.01680486325435 Accuracy 0.9718844984802432 Time 0.501s\n",
      "Epoch 2, Batch 330, Loss 10.013832219441731 Accuracy 0.9719696969696969 Time 0.501s\n",
      "Epoch 2, Batch 331, Loss 10.01333811491998 Accuracy 0.9719599697885196 Time 0.5s\n",
      "Epoch 2, Batch 332, Loss 10.015748905848307 Accuracy 0.9717620481927711 Time 0.501s\n",
      "Epoch 2, Batch 333, Loss 10.013643651395231 Accuracy 0.9718468468468469 Time 0.501s\n",
      "Epoch 2, Batch 334, Loss 10.01213039752252 Accuracy 0.9718375748502994 Time 0.501s\n",
      "Epoch 2, Batch 335, Loss 10.01327610300548 Accuracy 0.9718283582089552 Time 0.5s\n",
      "Epoch 2, Batch 336, Loss 10.012442634219216 Accuracy 0.9719122023809523 Time 0.501s\n",
      "Epoch 2, Batch 337, Loss 10.009447842040473 Accuracy 0.9719955489614244 Time 0.501s\n",
      "Epoch 2, Batch 338, Loss 10.01092742039607 Accuracy 0.9719859467455622 Time 0.501s\n",
      "Epoch 2, Batch 339, Loss 10.012479461399854 Accuracy 0.9718842182890856 Time 0.5s\n",
      "Epoch 2, Batch 340, Loss 10.011047862557804 Accuracy 0.9719669117647058 Time 0.502s\n",
      "Epoch 2, Batch 341, Loss 10.014537763735415 Accuracy 0.9718658357771262 Time 0.501s\n",
      "Epoch 2, Batch 342, Loss 10.011974945402981 Accuracy 0.9719480994152047 Time 0.501s\n",
      "Epoch 2, Batch 343, Loss 10.012835327459841 Accuracy 0.9719387755102041 Time 0.5s\n",
      "Epoch 2, Batch 344, Loss 10.011138979778734 Accuracy 0.9720203488372093 Time 0.501s\n",
      "Epoch 2, Batch 345, Loss 10.01386560080708 Accuracy 0.9719202898550725 Time 0.501s\n",
      "Epoch 2, Batch 346, Loss 10.012101845934211 Accuracy 0.9719111271676301 Time 0.501s\n",
      "Epoch 2, Batch 347, Loss 10.014261336422791 Accuracy 0.971721902017291 Time 0.5s\n",
      "Epoch 2, Batch 348, Loss 10.013816090835922 Accuracy 0.9715337643678161 Time 0.501s\n",
      "Epoch 2, Batch 349, Loss 10.011649410499201 Accuracy 0.971615329512894 Time 0.501s\n",
      "Epoch 2, Batch 350, Loss 10.012517975398472 Accuracy 0.9716071428571429 Time 0.501s\n",
      "Epoch 2, Batch 351, Loss 10.011197543891408 Accuracy 0.9715990028490028 Time 0.5s\n",
      "Epoch 2, Batch 352, Loss 10.00881376862526 Accuracy 0.9716796875 Time 0.501s\n",
      "Epoch 2, Batch 353, Loss 10.008542693032759 Accuracy 0.9717599150141643 Time 0.501s\n",
      "Epoch 2, Batch 354, Loss 10.004266429082149 Accuracy 0.9718396892655368 Time 0.501s\n",
      "Epoch 2, Batch 355, Loss 10.003518112612442 Accuracy 0.9719190140845071 Time 0.5s\n",
      "Epoch 2, Batch 356, Loss 10.000611050745075 Accuracy 0.971997893258427 Time 0.501s\n",
      "Epoch 2, Batch 357, Loss 10.00022941877862 Accuracy 0.9720763305322129 Time 0.501s\n",
      "Epoch 2, Batch 358, Loss 10.000730418626157 Accuracy 0.9720670391061452 Time 0.502s\n",
      "Epoch 2, Batch 359, Loss 10.000627523013144 Accuracy 0.9720577994428969 Time 0.506s\n",
      "Epoch 2, Batch 360, Loss 9.999827276335822 Accuracy 0.9721354166666667 Time 0.501s\n",
      "Epoch 2, Batch 361, Loss 10.000136150877891 Accuracy 0.9721260387811634 Time 0.5s\n",
      "Epoch 2, Batch 362, Loss 9.999255520204153 Accuracy 0.9722030386740331 Time 0.501s\n",
      "Epoch 2, Batch 363, Loss 9.994841249849514 Accuracy 0.9721935261707989 Time 0.5s\n",
      "Epoch 2, Batch 364, Loss 9.995546335702414 Accuracy 0.9721840659340659 Time 0.501s\n",
      "Epoch 2, Batch 365, Loss 9.993612294654323 Accuracy 0.9722602739726027 Time 0.501s\n",
      "Epoch 2, Batch 366, Loss 9.990616371071404 Accuracy 0.9723360655737705 Time 0.501s\n",
      "Epoch 2, Batch 367, Loss 9.985572983848302 Accuracy 0.9724114441416893 Time 0.5s\n",
      "Epoch 2, Batch 368, Loss 9.986053648202315 Accuracy 0.9723165760869565 Time 0.501s\n",
      "Epoch 2, Batch 369, Loss 9.986673587706031 Accuracy 0.9722222222222222 Time 0.5s\n",
      "Epoch 2, Batch 370, Loss 9.985033277563147 Accuracy 0.9722128378378379 Time 0.501s\n",
      "Epoch 2, Batch 371, Loss 9.98439680341119 Accuracy 0.9722877358490566 Time 0.5s\n",
      "Epoch 2, Batch 372, Loss 9.982132334862985 Accuracy 0.9723622311827957 Time 0.501s\n",
      "Epoch 2, Batch 373, Loss 9.981454695837108 Accuracy 0.972436327077748 Time 0.501s\n",
      "Epoch 2, Batch 374, Loss 9.979198873999284 Accuracy 0.9725100267379679 Time 0.501s\n",
      "Epoch 2, Batch 375, Loss 9.977325139363607 Accuracy 0.9725 Time 0.5s\n",
      "Epoch 2, Batch 376, Loss 9.974714025538018 Accuracy 0.9725731382978723 Time 0.501s\n",
      "Epoch 2, Batch 377, Loss 9.972564386119894 Accuracy 0.9724801061007957 Time 0.501s\n",
      "Epoch 2, Batch 378, Loss 9.974798439671753 Accuracy 0.9723048941798942 Time 0.501s\n",
      "Epoch 2, Batch 379, Loss 9.972532825922903 Accuracy 0.9723779683377308 Time 0.5s\n",
      "Epoch 2, Batch 380, Loss 9.967584429289165 Accuracy 0.9724506578947368 Time 0.501s\n",
      "Epoch 2, Batch 381, Loss 9.965956677914917 Accuracy 0.9725229658792651 Time 0.501s\n",
      "Epoch 2, Batch 382, Loss 9.964003585396012 Accuracy 0.9725948952879581 Time 0.501s\n",
      "Epoch 2, Batch 383, Loss 9.9641991084923 Accuracy 0.9724216710182768 Time 0.5s\n",
      "Epoch 2, Batch 384, Loss 9.9618696346879 Accuracy 0.9724934895833334 Time 0.501s\n",
      "Epoch 2, Batch 385, Loss 9.960028797001033 Accuracy 0.972564935064935 Time 0.501s\n",
      "Epoch 2, Batch 386, Loss 9.957001401970423 Accuracy 0.9725550518134715 Time 0.501s\n",
      "Epoch 2, Batch 387, Loss 9.956937614953487 Accuracy 0.9725452196382429 Time 0.5s\n",
      "Epoch 2, Batch 388, Loss 9.955754675816015 Accuracy 0.9726159793814433 Time 0.62s\n",
      "Epoch 2, Batch 389, Loss 9.95299692325543 Accuracy 0.9726863753213367 Time 0.635s\n",
      "Epoch 2, Batch 390, Loss 9.95065728456546 Accuracy 0.9727564102564102 Time 0.711s\n",
      "Epoch 2, Batch 391, Loss 9.949094523554264 Accuracy 0.9727461636828645 Time 0.682s\n",
      "Epoch 2, Batch 392, Loss 9.946818305521596 Accuracy 0.9728156887755102 Time 0.566s\n",
      "Epoch 2, Batch 393, Loss 9.947096676620212 Accuracy 0.9728053435114504 Time 0.512s\n",
      "Epoch 2, Batch 394, Loss 9.948795662313549 Accuracy 0.9727157360406091 Time 0.522s\n",
      "Epoch 2, Batch 395, Loss 9.946454441698291 Accuracy 0.9727056962025317 Time 0.5s\n",
      "Epoch 2, Batch 396, Loss 9.94650436651827 Accuracy 0.9726957070707071 Time 0.501s\n",
      "Epoch 2, Batch 397, Loss 9.946784300527884 Accuracy 0.9726070528967254 Time 0.651s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 398, Loss 9.946776615315347 Accuracy 0.972675879396985 Time 0.927s\n",
      "Epoch 2, Batch 399, Loss 9.948828164198643 Accuracy 0.9725877192982456 Time 0.5s\n",
      "Epoch 2, Batch 400, Loss 9.94851817369461 Accuracy 0.972578125 Time 0.561s\n",
      "Epoch 2, Batch 401, Loss 9.951963434195578 Accuracy 0.972568578553616 Time 0.501s\n",
      "Epoch 2, Batch 402, Loss 9.954516242392621 Accuracy 0.972481343283582 Time 0.501s\n",
      "Epoch 2, Batch 403, Loss 9.955279643719013 Accuracy 0.9724720843672456 Time 0.5s\n",
      "Epoch 2, Batch 404, Loss 9.95510471929418 Accuracy 0.9724628712871287 Time 0.501s\n",
      "Epoch 2, Batch 405, Loss 9.954061432826666 Accuracy 0.9725308641975309 Time 0.501s\n",
      "Epoch 2, Batch 406, Loss 9.955525630800595 Accuracy 0.9724445812807881 Time 0.501s\n",
      "Epoch 2, Batch 407, Loss 9.955826635149831 Accuracy 0.972512285012285 Time 0.5s\n",
      "Epoch 2, Batch 408, Loss 9.95685978496776 Accuracy 0.9725796568627451 Time 0.501s\n",
      "Epoch 2, Batch 409, Loss 9.959765660442175 Accuracy 0.9724938875305623 Time 0.5s\n",
      "Epoch 2, Batch 410, Loss 9.961298279645966 Accuracy 0.972484756097561 Time 0.501s\n",
      "Epoch 2, Batch 411, Loss 9.962213437342585 Accuracy 0.972551703163017 Time 0.787s\n",
      "Epoch 2, Batch 412, Loss 9.964486617486454 Accuracy 0.9726183252427184 Time 0.501s\n",
      "Epoch 2, Batch 413, Loss 9.965799941277677 Accuracy 0.9723819612590799 Time 0.501s\n",
      "Epoch 2, Batch 414, Loss 9.964595608089281 Accuracy 0.9724486714975845 Time 0.524s\n",
      "Epoch 2, Batch 415, Loss 9.964084820575025 Accuracy 0.9725150602409639 Time 0.5s\n",
      "Epoch 2, Batch 416, Loss 9.964415593789173 Accuracy 0.9723557692307693 Time 0.696s\n",
      "Epoch 2, Batch 417, Loss 9.965661412520374 Accuracy 0.9724220623501199 Time 0.501s\n",
      "Epoch 2, Batch 418, Loss 9.964809125690369 Accuracy 0.972488038277512 Time 0.501s\n",
      "Epoch 2, Batch 419, Loss 9.963612667985064 Accuracy 0.9725536992840096 Time 0.552s\n",
      "Epoch 2, Batch 420, Loss 9.966432108197893 Accuracy 0.9724702380952381 Time 0.555s\n",
      "Epoch 2, Batch 421, Loss 9.965866657447362 Accuracy 0.9725356294536817 Time 1.022s\n",
      "Epoch 2, Batch 422, Loss 9.96733197216739 Accuracy 0.9725266587677726 Time 0.501s\n",
      "Epoch 2, Batch 423, Loss 9.969424001995835 Accuracy 0.9725177304964538 Time 0.5s\n",
      "Epoch 2, Batch 424, Loss 9.972543050658029 Accuracy 0.9725088443396226 Time 0.501s\n",
      "Epoch 2, Batch 425, Loss 9.972753524780273 Accuracy 0.9725735294117647 Time 0.799s\n",
      "Epoch 2, Batch 426, Loss 9.971460841631107 Accuracy 0.9726379107981221 Time 0.793s\n",
      "Epoch 2, Batch 427, Loss 9.971483270792548 Accuracy 0.9726288056206089 Time 0.715s\n",
      "Epoch 2, Batch 428, Loss 9.968751185408262 Accuracy 0.9726197429906542 Time 0.608s\n",
      "Epoch 2, Batch 429, Loss 9.966533694233927 Accuracy 0.9726835664335665 Time 0.501s\n",
      "Epoch 2, Batch 430, Loss 9.966026947110198 Accuracy 0.9726744186046512 Time 0.501s\n",
      "Epoch 2, Batch 431, Loss 9.964256656142233 Accuracy 0.972737819025522 Time 0.658s\n",
      "Epoch 2, Batch 432, Loss 9.965193543169233 Accuracy 0.9727285879629629 Time 0.62s\n",
      "Epoch 2, Batch 433, Loss 9.966472634542347 Accuracy 0.9727193995381063 Time 0.756s\n",
      "Epoch 2, Batch 434, Loss 9.966323922856063 Accuracy 0.9727102534562212 Time 0.712s\n",
      "Epoch 2, Batch 435, Loss 9.964558450106917 Accuracy 0.9727729885057471 Time 0.838s\n",
      "Epoch 2, Batch 436, Loss 9.965107738424878 Accuracy 0.9727637614678899 Time 0.705s\n",
      "Epoch 2, Batch 437, Loss 9.966152658178823 Accuracy 0.9728260869565217 Time 0.675s\n",
      "Epoch 2, Batch 438, Loss 9.965599107960044 Accuracy 0.9728881278538812 Time 0.548s\n",
      "Epoch 2, Batch 439, Loss 9.969444894030318 Accuracy 0.9727363325740319 Time 0.604s\n",
      "Epoch 2, Batch 440, Loss 9.969414676319468 Accuracy 0.97265625 Time 0.78s\n",
      "Epoch 2, Batch 441, Loss 9.97143987841617 Accuracy 0.9726473922902494 Time 0.729s\n",
      "Epoch 2, Batch 442, Loss 9.97084101292882 Accuracy 0.9726385746606335 Time 0.698s\n",
      "Epoch 2, Batch 443, Loss 9.970636608907386 Accuracy 0.9727003386004515 Time 0.721s\n",
      "Epoch 2, Batch 444, Loss 9.971585776354816 Accuracy 0.9726914414414415 Time 0.709s\n",
      "Epoch 2, Batch 445, Loss 9.972637742288997 Accuracy 0.9726825842696629 Time 0.735s\n",
      "Epoch 2, Batch 446, Loss 9.975512720544241 Accuracy 0.9726036995515696 Time 0.81s\n",
      "Epoch 2, Batch 447, Loss 9.973213948926137 Accuracy 0.9726649888143176 Time 0.687s\n",
      "Epoch 2, Batch 448, Loss 9.976849206856318 Accuracy 0.97265625 Time 0.692s\n",
      "Epoch 2, Batch 449, Loss 9.977306731294153 Accuracy 0.9726475501113586 Time 0.708s\n",
      "Epoch 2, Batch 450, Loss 9.975336136288114 Accuracy 0.9727083333333333 Time 0.697s\n",
      "Epoch 2, Batch 451, Loss 9.977064392783426 Accuracy 0.9727688470066519 Time 0.703s\n",
      "Epoch 2, Batch 452, Loss 9.977733398960755 Accuracy 0.9727599557522124 Time 0.713s\n",
      "Epoch 2, Batch 453, Loss 9.978401569341193 Accuracy 0.972682119205298 Time 0.8s\n",
      "Epoch 2, Batch 454, Loss 9.977328907550694 Accuracy 0.9727422907488987 Time 0.69s\n",
      "Epoch 2, Batch 455, Loss 9.97631316132598 Accuracy 0.9727335164835165 Time 0.703s\n",
      "Epoch 2, Batch 456, Loss 9.976613676338864 Accuracy 0.9727247807017544 Time 0.714s\n",
      "Epoch 2, Batch 457, Loss 9.977449225127305 Accuracy 0.9726477024070022 Time 0.729s\n",
      "Epoch 2, Batch 458, Loss 9.977351313595168 Accuracy 0.97257096069869 Time 0.705s\n",
      "Epoch 2, Batch 459, Loss 9.97955488691143 Accuracy 0.9724264705882353 Time 0.741s\n",
      "Epoch 2, Batch 460, Loss 9.9795773278112 Accuracy 0.9724864130434783 Time 0.789s\n",
      "Epoch 2, Batch 461, Loss 9.978276519609894 Accuracy 0.9725460954446855 Time 0.909s\n",
      "Epoch 2, Batch 462, Loss 9.977474406684117 Accuracy 0.9726055194805194 Time 0.701s\n",
      "Epoch 2, Batch 463, Loss 9.977135217473007 Accuracy 0.972664686825054 Time 0.699s\n",
      "Epoch 2, Batch 464, Loss 9.976001527802698 Accuracy 0.97265625 Time 0.766s\n",
      "Epoch 2, Batch 465, Loss 9.97472995019728 Accuracy 0.9726478494623656 Time 0.702s\n",
      "Epoch 2, Batch 466, Loss 9.974203785089976 Accuracy 0.9727065450643777 Time 0.706s\n",
      "Epoch 2, Batch 467, Loss 9.971710080508247 Accuracy 0.9727649892933619 Time 0.698s\n",
      "Epoch 2, Batch 468, Loss 9.972773723113232 Accuracy 0.9728231837606838 Time 0.713s\n",
      "Epoch 2, Batch 469, Loss 9.971806623788277 Accuracy 0.9728811300639659 Time 0.773s\n",
      "Epoch 2, Batch 470, Loss 9.969885278255381 Accuracy 0.972938829787234 Time 0.622s\n",
      "Epoch 2, Batch 471, Loss 9.970304211740029 Accuracy 0.9728635881104034 Time 0.5s\n",
      "Epoch 2, Batch 472, Loss 9.974056177220103 Accuracy 0.972854872881356 Time 0.516s\n",
      "Epoch 2, Batch 473, Loss 9.974113288943913 Accuracy 0.9726479915433404 Time 0.501s\n",
      "Epoch 2, Batch 474, Loss 9.973444964815293 Accuracy 0.9725738396624473 Time 0.511s\n",
      "Epoch 2, Batch 475, Loss 9.974148947063245 Accuracy 0.9726315789473684 Time 0.578s\n",
      "Epoch 2, Batch 476, Loss 9.969931365061207 Accuracy 0.9726890756302521 Time 0.72s\n",
      "Epoch 2, Batch 477, Loss 9.972285509609327 Accuracy 0.9726808176100629 Time 0.707s\n",
      "Epoch 2, Batch 478, Loss 9.973612874122843 Accuracy 0.9726725941422594 Time 0.704s\n",
      "Epoch 2, Batch 479, Loss 9.974694485953059 Accuracy 0.9726644050104384 Time 0.604s\n",
      "Epoch 2, Batch 480, Loss 9.975133698185285 Accuracy 0.97265625 Time 0.501s\n",
      "Epoch 2, Batch 481, Loss 9.972933667117493 Accuracy 0.9727130977130977 Time 0.501s\n",
      "Epoch 2, Batch 482, Loss 9.973166799149572 Accuracy 0.9727697095435685 Time 0.58s\n",
      "Epoch 2, Batch 483, Loss 9.971330284331897 Accuracy 0.9728260869565217 Time 0.5s\n",
      "Epoch 2, Batch 484, Loss 9.97081748808711 Accuracy 0.9728176652892562 Time 0.519s\n",
      "Epoch 2, Batch 485, Loss 9.974489421451215 Accuracy 0.9727448453608247 Time 0.734s\n",
      "Epoch 2, Batch 486, Loss 9.973197771197974 Accuracy 0.9728009259259259 Time 0.686s\n",
      "Epoch 2, Batch 487, Loss 9.976886340969642 Accuracy 0.9727284394250514 Time 0.597s\n",
      "Epoch 2, Batch 488, Loss 9.976418642724147 Accuracy 0.9727843237704918 Time 0.744s\n",
      "Epoch 2, Batch 489, Loss 9.979181025413403 Accuracy 0.9727121676891616 Time 0.732s\n",
      "Epoch 2, Batch 490, Loss 9.978487722241148 Accuracy 0.9727678571428572 Time 0.776s\n",
      "Epoch 2, Batch 491, Loss 9.979257027146287 Accuracy 0.9728233197556008 Time 0.7s\n",
      "Epoch 2, Batch 492, Loss 9.978500062857217 Accuracy 0.9727515243902439 Time 0.831s\n",
      "Epoch 2, Batch 493, Loss 9.97794981660514 Accuracy 0.9728067951318459 Time 0.706s\n",
      "Epoch 2, Batch 494, Loss 9.978593159301079 Accuracy 0.9727985829959515 Time 0.717s\n",
      "Epoch 2, Batch 495, Loss 9.978541762419422 Accuracy 0.9727904040404041 Time 0.728s\n",
      "Epoch 2, Batch 496, Loss 9.977002986977178 Accuracy 0.9728452620967742 Time 1.06s\n",
      "Epoch 2, Batch 497, Loss 9.975181730220495 Accuracy 0.9728370221327968 Time 0.546s\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    'MNIST': torchvision.datasets.MNIST,\n",
    "    'CIFAR': torchvision.datasets.CIFAR10\n",
    "}\n",
    "\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "if not os.path.exists(DATA_PATH):\n",
    "  os.mkdir(DATA_PATH)\n",
    "\n",
    "args = {}\n",
    "args['dataset'] = 'MNIST'\n",
    "args['batch_size'] = 32\n",
    "args['lr'] = 0.005\n",
    "args['num_routings'] = 3\n",
    "args['lr_decay'] = 0.9\n",
    "args['data_path'] = DATA_PATH\n",
    "args['epochs'] = 4\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "loaders = {}\n",
    "trainset = datasets[args['dataset']](root=args['data_path'], train=True, download=True, transform=transform)\n",
    "loaders['train'] = torch.utils.data.DataLoader(trainset, batch_size=args['batch_size'], shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets[args['dataset']](root=args['data_path'], train=False, download=True, transform=transform)\n",
    "loaders['test'] = torch.utils.data.DataLoader(testset, batch_size=args['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "print ('==>>> total trainning batch number: {}'.format(len(trainset)))\n",
    "print ('==>>> total testing batch number: {}'.format(len(testset)))\n",
    "\n",
    "\n",
    "caps_net = CapsNetTrainer(loaders, args['batch_size'], args['lr'], args['num_routings'], args['lr_decay'], device=device)\n",
    "caps_net.run(args['epochs'], classes=classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4OMMOSuZtqWL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CapsNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
